---
title: MySQL 知识点
date: 2024-9-25 12:33:28
layout: post
categories: [MySQL知识点整理]
tags: [MySQL]
comments: true
cover: /img/mysql_logo.png
description: MySQL 整理知识点
---
# 1. MySQL 知识点 --- 索引                      

## 什么是数据库索引？索引的作用是什么？索引适用于哪些场景？

 数据库索引是一种 **数据结构**，用于提高数据库表的查询效率。

​		索引可以帮助数据库快速定位和检索存储在表中的数据，从而加快数据查询的速度。在数据量比较大时，使用索引可以极大地提高数据检索的效率。

​		索引的作用是通过构建一个额外的数据结构（**B+tree、哈希表等**）来加速数据的检索。它是在数据库表上创建的一种数据结构，**它包含一些指向表中数据的指针**，可以快速地定位到满足查询条件的数据行，从而提高查询效率。索引可以包含一个或多个列，可以使用 **单列索引、组合索引、全文索引** 等多种方式来创建。

**适合索引的场景包括：**

- 频繁查询的列，如主键、外键等。
- 经常作为查询条件的列，如 WHERE、ORDER BY 、GROUP BY 等语句中的列。
- 经常需要连接的列，如多表联合查询时的列。
- 数据量比较大的表，通过索引可以加快数据检索速度。

**索引的优点可以是提高数据库的查询速度，缩短数据检索的时间，提高系统的性能。但是索引也有一些缺点，包括：**

- 占用额外的存储空间，增加了存储技术。
- 建立索引需要时间，增加了系统的开销。
- 数据库的更新操作（增删改）会导致索引的重建，影响系统的性能。

因此，需要根据实际情况进行索引的创建和使用，避免过度索引导致系统性能下架。



## MySQL 中的覆盖索引和联合索引是什么？索引的最左前缀匹配原则是什么？

**覆盖索引** 和 **联合索引** 是数据库中常见的两种索引类型。

**覆盖索引** 是指一个 **包含了所有查询需要的列的索引** ，查询时可以直接从索引中取到需要的数据，而 **不需要再回到表中查找** ，从而可以提高查询效率。

**联合索引** 是指 **使用多个列组合起来作为一个索引** ，可以 **同时查询多个列** ，提高查询效率。联合索引可以包括多个列，但是查询时只能使用 **前缀列** 进行查询，即只有查询中使用了联合索引的前几个列，才能利用联合索引进行查询。如果查询中没有使用 **前缀列** ，那么 **联合索引就不能发挥作用** ，需要使用单独的索引或全表扫描。

**最左前缀匹配原则** 是指 **如果一个联合索引包含了多个列，那么在查询时只能使用前面的列进行匹配。**

例如：一个联合索引包含 A、B、C 三列，那么查询时只能用 A、AB 或 ABC 进行匹配，而不能只使用 B 或 C 进行匹配。这是因为如果查询时使用的列不是 **最左前缀列** ，那么 M有SQL 就无法使用索引进行查询，会导致全表扫描，从而降低查询效率。

在实际的应用中， **覆盖索引** 和 **联合索引** 可以结合使用，以提高查询效率。同时，使用 **最左前缀匹配原则** 可以让我们更加合理的设计索引，从而提高查询效率。



## MySQL 的索引类型有哪些？

 从 **数据结构** 角度来看，MySQL 索引可以分为以下几类：

- B+树索引
- 哈希索引
- 倒排索引（Full-Text）
- R-树索引（多维空间树）

从 **常见的基于 InnoDB B+树索引角度来看，** 可以分为：

- 聚簇索引（Clustered Index）
- 非聚簇索引（Non-clustered Index）

从 **索引性质** 的角度来看，可以分为：

- 普通索引（二级索引、辅助索引）
- 主键索引
- 联合索引
- 唯一索引
- 全文索引
- 空间索引

> 扩展知识：
>
> - B+tree 索引：B+tree 是 M有SQL 中最常用的索引类型。它使用平衡树来存储索引数据，适用于 **范围查询和排序**
> - 哈希索引：哈希索引基于哈希算法，将索引列的值转化为哈希值，然后通过哈希表来进行快速的等值查询。主要用于 Memory 引擎。查找速度非常快，但不支持范围查询， **只适用于精确的匹配查询**。
> - 倒排索引：用于对文本字段进行全文搜索，允许文本数据中进行关键字搜索和模糊匹配。
> - R-树索引：空间索引用于处理空间数据，如地理位置和几何形状。MySQL 提供了专门的空间索引类型，可以用于空间数据的查询和分析。



## MySQL InnoDB 引擎中的聚簇索引和非聚簇索引有什么区别？

聚簇索引：

- 索引叶子结点存储的是数据行，可以直接访问完整数据。
- 每个表只能有一个聚簇索引，通常是主键索引，适合范围查询和排列。

非聚簇索引：

- 索引叶子节点存储的是数据行的主键和对应的索引列，需要通过主键才能访问完整的数据行。
- 一个表可以有多个聚簇索引（称之为非主键索引、辅助索引、二级索引），适用于快速查特定列的数据。



## MySQL 索引的最左前缀匹配原则是什么？

MySQL 索引的最左前缀匹配原则是指的是在适用 **联合索引** 时，查询条件必须从索引的最左侧开始匹配。如果一个联合索引包含多个列，查询条件必须包含第一个列的条件，然后是第二个列，以此类推。



## MySQL 的覆盖索引是什么？

MySQL 的覆盖索引（Covering Index）是指二级索引中包含了查询所需要的所有字段，从而使查询可以仅通过访问二级索引而不需要访问实际的表数据（主键索引）。

> 扩展知识：
>
> - 减少 I/O 操作：因为查询可以直接从索引中获取所有需要的数据，避免了访问实际表的数据页，从而减少了 I/O 操作。
> - 提高查询速度：索引比表数据更紧凑，因此从索引中读取数据比从表中读取要快。
> - 减少内存占用：只要读取索引页而不是表数据页，可以减少内存占用。



## MySQL 的索引下推是什么？

索引下推（Index Condition Pushdown ICP）是一种减少回表查询，提高查询效率的技术。它允许 MySQL 在使用索引查找数据时，将部分查询条件下推到索引存储引擎层过滤，从而减少需要从表中获取的数据行，减少了 IO （本该由 Service 层做操作，交由存储引擎层因此叫做 “下推”）。

**注意：索引下推是应用在联合索引**。

> 扩展知识：
>
> - 索引下推在 M有SQL 5.6 及以后的版本支持， InnoDB 和 M有ISAM 这两个存储引擎都生效。
> - 如果查询中引用了子查询索引下推可能不会生效，具体看 explain。
> - 使用了函数或表达式索引下推也不会生效，这个和是否命中索引的条件是一样的。
> - 使用了聚簇索引（主键）查询，索引下推也不会生效，因为其是对于非聚簇索引来进行减少回表次数。



## 在 MySQL 中建立索引时需要注意哪些事项？

1. 不能盲目建立索引，**索引并不是越多越好** ，索引会占用空间，且每次修改的时间可能都需要维护索引的数据，消耗资源。
2. 对于字段的值有 **大量重复的不要建立索引**。 比如说：性别字段，在这种重复比例很大的数据行中，建立索引页不能提高检索速度。**但是也不绝对** ，例如定时任务的场景，大部分任务都是成功的，少部分任务状态是失败的，这时候通过失败状态去查询任务，实际上能过滤大部分成功的任务，效率还是可以的。
3. 对于一些 **长字段不应该建立索引**。比如 text、longtext 这种类型字段不应该建立索引。因为占据的内存大，扫描的时候大量加载至内存中还耗时，使得提升性能可能不明显，甚至可能还会降低整体的性能，因为别的缓存数据可能因为它被踢出内存，下次查询还需要从磁盘中获取。
4. 当 **数据表的修改频率远大于查询频率** 时，应该好好考虑是否需要建立索引。因为建立索引会减慢修改的频率，如果很少的查询较多的修改，则得不偿失。
5. 对于需要频繁作为条件查询的字段应该建立索引。在 where 关键词后经常查询的字段，建立索引能提高查询的效率，如果有多个条件经常一起查询，则 **可以考虑联合索引，减少索引数量**。
6. 对经常在 order by 、group by 、distinct 后面的字段建立索引。这些操作通常需要对结果进行排序、分组或者去重，而索引可以帮助加快这些操作的速度。



## MySQL 中使用索引一定有效吗？如何排查索引效果？

索引不一定有效：

- 在某些情况下，索引可能不会被使用，例如查询条件中涉及的列未被索引、低基数列索引效果不佳，或查询条件复杂且不匹配索引的顺序。
- 对于小表，MySQL 可能选择全表扫描而非使用索引，因为全表扫描的开销可能更小。
- 总而言之，最终是否用上索引是根据 MySQL 成本计算决定的，评估 CPU 和 I/O 成本最终选择用辅助索引还是全表扫描。有时候确实是全表扫描成本低所以没用上索引。但有时候由于一些统计数据的不准确，导致陈本计算误判，而没用上索引。

排查索引效果的方法：

- 使用 **EXPLAIN** 命令：通过在查询前加上 **EXPLAIN** ，可以查看 MySQL 选择的执行计划，了解是否使用了索引、使用了哪个索引、估算的行数等信息。

主要观察 **EXPLAIN** 结果以下几点：

- type（访问类型）：这个属性显示了查询使用的访问方法，例如 **ALL、index 、rang** 等。当查询使用索引时，这个属性通常会显示为 **index** 或 **range** ，表示查询使用了索引访问。如果这个值是 **ALL** ，则表示查询执行了全表扫描，没有使用索引。
- key（使用的索引）：这个属性显示了查询使用的索引，如果查询使用了索引，则会显示索引的名称。如果中国值是 **NULL** ，则表示查询没有使用索引。
- rows （扫描的行数）：这个属性显示了查询扫描的行数，即查询返回的行数，需要评估下扫描量。

> 知识扩展：
>
> **索引失效的场景：**
>
> 1. 使用了联合索引却不符合最左前缀
>
> 2. 索引中使用了运算符
>
> 3. 索引上使用了函数也会失效
>
> 4. like 的随意使用
>
> 5. or 的随意使用
>
> 6. 随意的字段类型使用
>
> 7. 不同的参数也会导致索引失效
>    （MySQL会自动推荐是否需要走索引，如一个参数，数据量是表的 50% 以上，MySQL 会选择走全表查询，效率会更高）
>
> 8. 表中两个不同的字段进行比较
>
> 9. 使用了 != 、<>
>
> 10. 使用了 is not null
>
> 11. 使用了 order by
>
>     （当 order by 后面跟的 **不是主键** 或者 **不是覆盖索引** 会导致不走索引）
>
> **为什么索引生效了反而查询变慢了？**
>
> 1. 确认是否选对了索引！ MySQL 根据优化器会评估成本选择对应的索引，但有时候 MySQL 因为估计值不准确，导致选错了索引，因此查询速度反而更慢。
> 2. 对大字段建立了索引，给 text 字段建立索引导致查询需要扫描更多的数据块，反而变慢。



## MySQL 中的索引数量是否越多越好？为什么？

> 简答：
>
> ​	并不是越多越好，因为索引页需要内存空间进行维护，频繁的修改索引字段还需要维护索引的更新，其次索引多了，优化器在选择是否走索引的时候也会消耗更多的时间来进行抉择。

索引并不是越多越好，因为索引不论从时间还是空间上都是有一定成本的

1. 从时间上：

   ​		每次对表中数据进行增删改的时候，索引也必须被更新，这会增加写入操作的开销。例如删除一个 name 为 林羽 的记录，不仅主键索引上需要修改，如果 name 字段有索引，那么 name 索引也需要修改，所以 **索引越多需要修改的地方就越多，时间开销就大了** ，并且 B+ 树可能会有页分裂、合并等操作，时间开销就会更大。

   ​		还有一点需要注意：MySQL 有个查询优化器，它需要分析当前的查询，选择最优的计划，这过程就需要考虑选择哪个索引的查询成本低。如果索引过多，那么会导致优化器耗费更多的时间在选择上，甚至可能因为数据不准确而选择了次优的索引。

2. 从空间上：

   每建立一个二级索引，都需要新建一个 B+ 树，默认每个数据页都是 16kb，如果数据量很大，索引又很多，占用的空间可不小。



## 为什么 MySQL 选择使用 B+ 树作为索引结构？

> 简答：
>
> - 首先 B+ 树 是自平衡树，在新增和删除节点时会自平衡，也有冗余节点，使得删除的效率增加。
> - 其次，B+ 树是矮胖型的，不像红黑树数据越多高多就越高，磁盘 IO 次数少。
> - 还有，叶子节点使用双向链表链接，便于范围查找

B+ 树在数据库系统中具有以下几个明显的优势：

1. 高效的查找性能：

   ​		B+ 树是一种自平衡树，每个叶子节点到根节点的路径长度相同，B+ 树在插入和删除节点时会进行分裂和合并操作，以保持树的平衡，但它又会有一定的冗余节点，使得删除的时候树结构的变化小，更高效。

   ​		查找、插入、删除等操作的时间复杂度为O（logn），能够保证在大数据量情况下也能有较快的响应时间。

2. 树的高度增长不会过快，使得查询磁盘的 I/O 次数减少：

   ​		B+ 树不像红黑树，数据量越多树的高度增长就越快。它是多叉树， **非叶子节点仅保存主键或索引值和页面指针** ，使得每一页能容纳更多的记录，因此内存中就能存放更多的索引，容易命中缓存，使得查询磁盘的 I/O 次数减少。

3. 范围查询能力强：

   ​		B+ 树特别适合范围查询。因为叶子节点通过链表链接，从根节点定位到叶子节点查找的范围的起点后，只需要顺序扫描链表即可遍历后续的数据，非常高效。 

> 扩展知识：
>
> **B+ 树和 B 树的区别：**
>
> - B 树每个节点都存储了完整的数据，而 B+ 树非叶子节点仅存储 key 和指针，完整数据存储在叶子节点。这使得 B+ 树可以在内存中存放更多索引页、减少磁盘查询次数。
> - B+ 树叶子组成了链表，便于区间查找，而 B 树只能每一层遍历查找。
> - B+ 树查询时间更平均、稳定，都需要从根节点扫描到叶子节点。而 B 树则在非叶子节点就可能找到对应的数据返回。



## 在什么情况下，不推荐为数据库建立索引？

> 简答：
>
> 1. 频繁进行增删改的列 --> 增删改的同时需要维护索引，消耗大。
> 2. 高度重复的列。
> 3. 查询频率比较低的列。
> 4. 非常长的列，比如 TEXT 类型这种 --> 占用磁盘空间、索引加载到内存，占用大量内存；数据量大，一个页能存储的行就很少，就会导致频繁需要去磁盘加载其它的行，大量的 I/O。

一般有以下几种：

1. 对于数据量很小的表：
   - 当表的数据很小（如几百条记录）时，建立索引并不会显著提升查询性能，反而可能增加管理的复杂性。
2. 频繁更新的表：
   - 对于频繁进行插入、更新和删除操作的表，索引会导致额外的维护开销，因为每次数据变更时都需要更新索引，这会影响性能。
3. 执行大量的 SELECT * 
   - 此时二级索引可能不会显著提升性能，因为需要大量的回表查询，开销大，数据库最终可能会选择走全表扫描。
4. 低选择性字段（高度重复的列）
   - 当索引字段的取值重复度高（如性别字段"男"、“女”），索引的效果不明显，且会增加存储空间的浪费。
   - 但是，还有一种场景可以考虑，比如表里任务 status 列就 2 个类型，90% 都是 1 （已完成）， 10% （待执行）是 2，这个场景就会频繁查询 2（待执行）的任务来执行，此时可以建立索引，毕竟能过滤 90% 的数据。
5. 低频查询的列
   - 对于查询频率极低的字段，建立索引的成本和维护负担可能超过带来的性能提升。
6. 长文本字段（非常长的 varchar 或 JSON 、 BLOB 和 TEXT 类型，这些类型的列通常包含大量的数据）
   - 数据量大排列时都无法用内存排，只能利用磁盘文件，排序很慢。
   - 数据量大，每个页能存放的行数就少，扫描查询可能会涉及大量的 I/O。
   - 文本字段过大都需要额外的 blob 页存储，每次查询还需要额外的页，也是随机 I/O 效率低。
   - 这种类型的数据如果有查询需求，不应该放到 MySQL 中，可以需要采用 es 等组件来实现查询。



## 如何在 MySQL 中优化索引以提高查询性能？

1. 选择合适的索引类型：
   - B树索引：适合大部分一般的查询操作。
   - 全文索引：主要用来对文本进行快速的全文搜索。
   - 哈希索引：主要用于精确匹配查询。
2. 减少索引数量：
   - 虽然索引能提高查询速度，但过多的索引会影响写入性能。因此，应该在满足需求的前提下，尽量减少索引的数量。
3. 选择合适的字段建立索引：
   - 应该在经常用于 WHERE 子句、JSON 子句 和 ORDER BY 子句中的字段上建立索引。
4. 使用复合索引：
   - 在多个字段上构建复合索引，尽量让查询条件中使用的字段顺序根索引定义中的字段顺序一致。
5. 避免低选择性列的索引：
   - 对于选择性较低的列（例如：性别、状态等），建立索引的效果会较差。
6. 利用覆盖索引：
   - 尽量让查询的字段包含在索引中，这样可以避免回表操作，提高查询速度。

> 拓展知识：
>
> **除了上述方法，还可以结合以下方式来进一步优化 MySQL 查询性能：**
>
> 1. 分析查询性能：
>    - 使用 EXPLAIN 语句来分析查询的执行计划，可以帮助我们了解查询的运行情况以及是否有效利用了索引。
> 2. 定期维护索引：
>    - 使用 OPTIMIZE TABLE 来进行表的维护，例如碎片整理，可以帮助维持索引的性能。
> 3. 合理设计数据库表结构：
>    - 例如，选择正确的数据类型可以减小表的大小，从而提高查询性能。
> 4. 缓存机制：
>    - 利用 MySQL 的查询缓存或者 Redis 等外部缓存机制，可以显著提升查询响应速度。
> 5. 分区和分片：
>    - 对大表进行分区或分片操作，可以大幅度减小查询的数据范围，从而提高查询性能。
> 6. 批量插入和更新：
>    - 在进行大批量数据操作时，尽量进行批量操作而不是一条条操作，可减少索引更新的开销。



## MySQL 中的 Full-Text Search 索引如何优化全文检索？

在 MySQL 中 ，Full-Text Search（全文索引）索引是针对文本数据快速执行复杂搜索的利器。它通过为大块文本数据建立索引用于加速查询。要优化全文检索查询，可以从如下方式入手：

1. 创建合适的 Full-Text 索引。
2. 使用适当的查询语法，比如 MATCH...AGAINST
3. 调整 MySQL 配置参数，如： innodb_ft_min_token_size 和 ft_max_word_len
4. 使用查询扩展和布尔模式来优化搜索。
5. 定期维护并优化数据库表，比如更新统计信息和重建索引。





# 2. MySQL 知识点 --- MVCC                      

## 基础理解

> 简答：
>
> ​		MVCC（Multi-Version Controller Control，多版本并发控制），可以说是**无锁**的并发控制，指的是在使用 read-committed读已提交、repeatable-read可重复读这两种隔离级别的事务，在执行普通的 SELECT 操作时，访问记录的版本链的过程。是一种并发控制机制，允许多个事务读取和写入数据库，而无需互相等待，从而提高数据库的并发能力。
>
> ​		本质上是借助 undolog 记录每次写操作的反向操作。
>
> ​		其中，read-committed（读已提交）、repeatable-read（可重复读）这两个隔离级别最大的不同点就是：生成 ReadView 的时机不同。
>
> - read-committed 在每次进行普通 select 操作前都会生成一个 ReadView；
> - repeatable-read 只在第一次进行普通 select 操作前生成一个 ReadView，之后的查询操作都重复复使用这个 ReadView 就好了。
>
> // MVCC 不可避免幻读情况，在事务开启的时候直接使用 **select ... for update** 加锁之后其它事务就新增不了数据量，也就避免了幻读的发生。

​		MVCC （Multi-Version Concurrency Control，多版本并发控制）是一种并发控制机制，允许多个事务同时读取和写入数据库，而无需互相等待，从而提高数据库的并发能力。

​		在 MVCC 中，数据库为每个事务创建一个数据快照。每当数据被修改时，MySQL 不会立即覆盖原有数据，而是生成新版本的记录。每个记录都保留了对应的版本号或时间戳。

​		多版本之间串联起来就形成了一条版本链，这样不同时刻启动的事务可以 **无锁** 地获得不同版本的数据（普通读）。此时读（普通读）写操作不会阻塞。

写操作可以继续写，无非就是会创建新的数据版本（只有在事务提交后，新版本才会对其他事务可见。未提交的事务修改不会影响其他事务的读取），历史版本记录可供已经启动的事务读取。

![MySQL MVCC](/img/0001-MySQL_MVCC.png "MySQL MVCC示意图")

> 扩展知识：
>
> **Undo Log：**
>
> Undo Log 是 MySQL InnoDB 中用于支持事务回滚操作的一种日志机制。它记录了数据修改的历史信息，使得在事务失败或需要撤回某些操作时，可将数据恢复到先前的状态。
>
> 实际上 MVCC 所谓的多版本不是真的存储了多个版本的数据， **只是借助 undolog 记录每次写操作的反向操作** ，所以索引上对应的记录只会有一个版本，即最新版本。只不过可以根据 undolog 中的记录反向操作得到数据的历史版本，所以看起来是多版本。
>
> ![MySQL MVCC](/img/0002-MySQL_undolog.png "MySQL MVCC示意图")
>
> 拿 **insert(1, xx)** 这条语句举例，成功插入之后数据页的记录不仅存储 ID 1；name XX，还有 trx_id 和 roll_pointer 这两个隐藏字段；
>
> - trx_id：当前事务 ID。
> - roll_pointer：指向 undo log 的指针。
>
> ![MySQL MVCC](/img/0003-undolog_插入隐藏字段.png "MySQL MVCC示意图")
>
> 从图中可以得知此时插入的事务 ID 是 1，此时插入会生成一条 undolog，并且记录上的 roll_pointer 会指向这条 undolog，而这条 undolog 是一个类型为 **TRX_UNDD_INSERT_REC** 的 log，代表是 insert 生成的。
>
> 里面存储了主键的长度和值（还有其他值，不提），所以 InnoDB 可以根据 undolog 里的主键的值，找到这条记录，然后把它删除来实现回滚（复原）的效果。因此可以简单地理解 undolog 里面存储的就是当前操作的反向操作，所以认为里面存了个 **delete 1** 就ok了。
>
> 此时 **事务 1 提交**，然后另一个 ID 为 5 的事务再执行 **update NO where id 1** 这个语句，此时的记录和 undolog 就如下图所示：
>
> ![MySQL MVCC](/img/0004-update.png "MySQL MVCC示意图")
>
> 没错，之前 insert 产生的 undolog 没了，insert 的事务提交了之后对应的 undolog 就回收了，因为不可能有别的事务会访问比这还要早的版本了，访问插入之前的版本？访问个寂寞吗？
>
> 而 update 产生的 undolog 不一样，它的类型为 **TRX_UNDO_UPD_EXIST_REC** 。
>
> 此时 **事务 5 提交** ，然后另一个 ID 为 11 的事务执行 **update Yes where id 1** 这个语句，此时的记录和 undolog 就如下图所示：
>
> ![MySQL MVCC](/img/0005-id11.png "MySQL MVCC示意图")
>
> 没错，update 产生的 undolog 不会马上删除，因为可能有别的事务需要访问之前的版本，所以不能删。这样就串成了一个版本链，可以看到记录本身加上两条 undolog，这条 id 为 1 的记录共有三个版本。
>
> **readView：**
>
> 版本链搞清楚了，这时候还需要知道一个概念 readView，这个 readView 就是用来判断哪个版本对当前事务可见的，这里有四个概念：
>
> 1. creator_trx_id：当前事务 ID。
> 2. m_ids：生成 readView 时还活跃的事务 ID 集合，也就是已经启动但是还未提交的事务 ID 列表。
> 3. min_trx_id：当前活跃 ID 之中的最小值。
> 4. max_trx_id：生成 readView 时 InnoDB 将分配给下一个事务的 ID 的值（事务 ID 是递增分配的，越后面申请的事务 ID 越大）。
>
> **对于可见版本的判断是从最新版本开始沿着版本链逐渐寻找老的版本，如果遇到符合条件的版本就返回。**
>
> 判断条件如下：
>
> - 如果当前数据版本的 trx_id == creator_trx_id 说明修改这条数据的事务就是当前的事务，所以可见。
> - 如果当前数据版本的 trx_id < min_trx_id，说明修改这条数据的事务在当前事务生成 readView 的时候已提交，所以可见。
> - 如果当前数据版本的 trx_id 大小在 min_trx_id 和 max_trx_id 之间，此时 trx_id 若在 m_ids 中，说明修改这条数据的事务此时还未提交，所以不可见，若不在 m_ids 中，表明事务已经提交，可见。
> - 如果当前数据版本的 trx_id >= max_trx_id，说明修改这条数据的事务在当前事务生成 readView 的时候还未启动，所以不可见（结合事务 ID 递增来看）。
>
> （在不同的隔离级别下案例见：https://www.mianshiya.com/bank/1791003439968264194/question/1780933295484203009 ）



## 如果 MySQL 中没有 MVCC，会有什么影响？

如果没有 MVCC，在频繁读写操作下要保证数据的正确性就必须进行加锁操作，因为**增加了锁的获取和释放的开销**，会导致整体系统响应速度变慢，降低了系统的并发处理能力。（这种实现叫：LBCC ---> Lock-Based Concurrent Control）

------



# 3. MySQL 知识点 --- 事务

## MySQL 是如何实现事务的？

- MySQL 主要通过：**锁、Redo Log、Undo Log、MVCC** 来实现事务的。
- MySQL 利用锁（行锁、间隙锁等等）机制，使用数据并发修改的控制，满足事务的隔离性。
- Redo Log（重做日志），它会记录事务对数据库的所有修改，在崩溃时恢复未提交的更改，用来满足事务的**持久性**。
- Undo Log（回滚日志），它会记录事务的反向操作，简单地说就是保存数据的历史版本，用于事务的回滚，使得事务执行失败之后可以恢复之前的样子。实现**原子性**和**隔离性**。
- MVCC（多版本并发控制），满足非锁定读的需求，提高了并发度，实现了读已提交和可重复读两种隔离级别，实现了事务的隔离性。
- 其实事务主要是为了实现一致性的，具体是通过 AID ，即原子性、隔离性 和 持久性来达到一致性的目的。



## MySQL 中长事务可能会导致哪些问题？

1. 长时间的锁竞争，阻塞资源：
   - 长事务持有的锁时间较长，容易导致其他事务在尝试获取锁时发生阻塞吗，从而增加系统的等待时间和降低并发性能。
   - 业务线程长时间等待数据库的请求等待而阻塞，部分业务的阻塞可能还会影响到别的服务，导致产生雪崩，最终使得服务全面崩盘，导致非常严重的线上事故。
2. 死锁风险：
   - 长事务更容易产生死锁，因为多个事务可能在互相等待对方释放锁，导致系统无法继续执行。
3. 主从延迟：
   - 主库需要长时间执行，然后传输给从库，从库又要重放好久，期间可能有很长一段时间数据是不同步的。
4. 回滚导致时间浪费：
   - 如果长事务执行很长一段时间，中间突发状况导致抛错，使得事务回滚了，之前做的执行都浪费了。

> 扩展知识：
>
> **长事务的 SQL 如何处理？**
>
> 具体看：https://www.mianshiya.com/bank/1791003439968264194/question/1780933295480008706



## MySQL 中的事务隔离级别有哪些？

MySQL中的事务隔离级别主要分为以下四种：

- **读未提交（READ UNCOMMITTED）**    ----- 最低的隔离级别

  事务 a 可以读取到事务 b 未提交的数据，会出现**脏读**的现象。

- 读已提交（READ COMMITTED）  ko **解决脏读问题**

  事务 a 可以读取到事务 b 已经提交的数据，会出现**不可重复读**的现象，提交前后多次读取，数据不一致。

- 可重复读（REPEATABLE READ）  ko **解决不可重复读问题**

  事务 a 可以读取到相同的数据，可能会出现**幻读**现象，前后多次读取，数据前后总量不一致。

- 串行化（SERIALIZABLE）  ---- 最高的隔离级别

  每个事务都会等待前一个事务执行完毕才会执行，这可避免所有的并发问题，但是会大大**降低并发性能。**

  

## MySQL 默认的事务隔离级别是什么？为什么选择这个级别？

MySQL 默认的隔离级别是**可重复读**（Repeatable Read），即 RR。

原因是为了兼容早期 binlog 的 statement 格式问题，如果是使用读已提交、读未提交等隔离级别，使用了 statement 格式的 binlog 会导致主从（备）数据库数据不一致问题。

如：事务 a 先执行 delete 但未提交，事务 b 在事务 a 执行 delete 时间线后执行了 insert 先提交了，然后事务 a 

再提交。binlog 的记录顺序是 insert delete，在从库同步 binlog 重放的时候，先执行 insert 再执行 delete，**这会导致从库的数据与主库不一致！**为了避免这个问题，默认为可重复读级别的。

> 扩展知识:
>
> **分析 binlog statement 格式 和可重复级别的影响：**
>
> 详见：https://www.mianshiya.com/bank/1791003439968264194/question/1780933295492591618



# 4. MySQL 知识点 --- 锁                     

## MySQL 中有哪些锁类型？

1. 行级锁（Row Lock）**重点**

- 仅对特定的行加锁，允许其他事务并发访问不同的行，适用于高并发场景。

2. 表级锁（Table Lock）**重点**、

- 对整个表加锁，其他事务无法对该表进行任何读写操作，适用于需要保证完整的小型表。

3. 意向锁（Intention Lock）

- 一种表锁，用于表示某个事务对某行数据加锁的意图，分为意向锁（IS）和意向排它锁（IX），主要用于行锁与表级锁的结合。

4. 共享锁（Shared Lock）**重点**

- 允许多个事务并发读取同一资源，但不允许修改。只有在释放共享锁后，其他事务才能获得排它锁。

5. 排它锁（Exclusive Lock）**重点**

- 只允许一个事务对资源进行读写，其他事务在获得排它锁之前无法访问该资源。

6. 元数据锁（Metadata Lock，MDL）

- 用于保护数据库对象（如表和索引）的元数据，防止在进行 DDL 操作时其他事务对这些对象进行修改。

7. 间隙锁（Gap Lock）**重点**

- 针对索引中两个记录之间的间隙加锁，防止其他事务在这个间隙中插入新纪录，以避免幻读。间隙锁不锁定具体行，而是锁定行与行之间的空间。

8. 临键锁（Next-Key Lock）**重点**

- 是行级锁和间隙锁的结合，锁定具体行和其前面的间隙，确保在一个范围内不会出现幻读。常用于支持可重复读的隔离级别。

9. 插入意向锁（Insert Intention Lock）

- 一种特殊的意向锁，用于指示事务打算在某个间隙中插入记录，允许其他事务进行共享锁，但在插入时会阻止其他的排它锁。

10. 自增锁（Auto Increment Lock）

- 在插入自增列时，加锁以保证自增值的唯一性，防止并发插入导致的冲突。通常在插入操作时被使用，以确保生成自增 ID 是唯一的。



## MySQL 的乐观锁和悲观锁是什么？

- 悲观锁：每次读写操作直接默认加锁；适合：**并发冲突多，写多读少的场景**。
- 乐观锁：假设不会发生冲突，因此在操作的时候不加锁，而是在更新数据的时 **进行版本控制 或 校验**；适合：**并发冲突少，读多写少的场景**。



## MySQL 中如果发生死锁应该如何解决？

1. **自动检测与回滚：**

- MySQL 自带死锁检测机制（innodb_deadlock_detect），当检测到死锁时，数据库会自动回滚其中一个事务。以解除死锁。通常会回滚事务中持有最少资源的那个。
- 也有锁等待超时的参数（innodb_lock_wait_timeout）,当获取锁的等待时间超过阈值，就会释放锁进行回滚。

2. 手动 kill 发生死锁的语句：

- 可以通过命令，手动快速地找出被阻塞的事务及其线程 ID，然后手动的 kill 它，及时释放资源。

> 扩展知识：
>
> **常见避免死锁或降低死锁的手段：**
>
> - **避免大事务**：大事务占据锁的时间长，将事务拆分为多个事务快速释放锁，可降低死锁产生的概念和避免冲突。
> - **调整申请锁的顺序**：在更新数据的时候要保证获得足够的锁，举个例子：先获取影响范围大的锁，比如说修改操作，先将排他锁获取到，再获取共享锁。或固定顺序访问数据，这样也能避免死锁的情况。
> - **更改数据库隔离级别**：可重复读比读已提交多了间隙锁和临键锁，利用读已提交替换之可降低死锁的情况。
> - **合理建立索引，减少加锁范围**：如果命中索引，则会锁对应的行，不然就是全表行都加锁，这样冲突大，死锁的概率就高了。
> - **开启死锁检测**：适当调整锁等待时长。

------

# 5. MySQL 知识点 --- 主从架构

## 如何在 MySQL 重避免单点故障？

一般会使用 **主从架构** 来避免单点故障，主数据库处理写操作，从数据库处理读操作，主数据库故障时可以切换到从数据库。

同时会对数据进行定期备份并存储在不同的物理位置，以便在发生故障时能够快速恢复数据。

并且需要建立监控系统，实时监控数据库的健康状态，并在发生故障时及时告警。

> 扩展知识：
>
> MySQL 几种架构介绍：
>
> - **主备架构**：主机 + 备机，备机不干活（不对外提供服务），只是默默人同步主机的数据，当主机挂了，可以用备机取代。
>
>   切换方式有两种：
>
>   - 人工切换：得知主机挂了之后手动把备机切成主机，缺点就是慢。
>   - 利用 keepalived 或者主机写给脚本来监控，然后自动切换。
>
> - **主从架构**：主机和从机，读写分离，主机写，从机读。
>
> - **主主架构**：两个主机。（**一般情况都不会有主主架构的，两个写，互相同步数据，会造成覆盖**）



## 如何在 MySQL 中实现读写分离？

1. **做法一：代码封装**

讲白了就是代码层面抽出一个中间层，由中间层来实现读写分离和数据库连接。

利用代理类，对外暴露正常的读写接口，里面封装了逻辑，将读操作指向从库的数据源，写操作指向主库的数据源。

- 优点：简单，并且可以根据业务定制化变化，随心所欲。
- 缺点：如果数据库宕机了，发生主从切换之后，就得修改配置重启。如果系统是多语言的话，需要为每个语言都实现一个中间层代码，重复开发。

2. **做法二：使用中间件**

中间件一般而言是独立部署的系统，客户端与这个中间件的交互是通过 SQL 协议的。

所以在客户端看来连接的就是一个数据库，通过 SQL 协议交互也可以屏蔽多语言的差异。

缺点就是整体架构多了一个系统需要维护，并且可能成为性能瓶颈，毕竟交互都需要经过它中转。

常见的开源数据库中间件有：官方的 MySQL-Proxy、360 的 Atlas、ShardingSphere、Mycat等。

> 扩展知识：
>
> **什么是读写分离：**
>
> - 读写分离就是读操作和写操作从以前的一台服务器上剥离开来，将主库压力分担一些到从库。本质上是因为访问量太大，主库的压力过大，单机数据库无法支撑并发读写。然后一般而言读的次数远高于写，因此将读操作分发到从库上，这就是常见的读写分离。
> - 读写分离还有个操作就是主库不建查询的索引，从库建查询的索引。因为索引是需要维护的，比如你插入一条数据，不仅要在聚簇索引上面插入，对应的二级索引也得插入，修改也是一样的。所以将读操作分到从库了之后，可以在主库把查询要用的索引删了，减少些操作对主库的影响。



## 什么是 MySQL 的主从同步机制？它是如何实现的？

MySQL 的主从同步机制是一种数据复制技术，用于将主数据库（Master）上的数据同步到一个或多个从数据库（Slave）中。

主要是通过二进制日志（Binary Log，简称 binlog）实现数据的复制。主数据库在执行写操作时，会将这些操作记录到 binlog 中，然后推送给从数据库，从数据库重放对应的日志即可完成复制。

> 扩展知识
>
> **MySQL 主从复制类型**
>
> MySQL 支持异步复制、同步复制、半同步复制
>
> - **异步复制**：主库不需要等待从库的响应（性能较高，数据一致性较低）
> - **同步复制**：主库同步等待所有从库确认收到数据（性能差，数据一致性高）
> - **半同步复制**：主库等待至少一个从库确认收到数据（性能折中，数据一致性较高）
>
> **异步复制**
>
> MySQL 默认是异步复制，具体流程如下：
>
> 主库：
>
> - 接受到提交事务请求
> - 更新数据
> - 将数据写到 binlog 中
> - 给客户端响应
> - 推送 binlog 到从库中
>
> 从库：
>
> - 由 I/O 线程将同步过来的 binlog 写入到 relay log 中
> - 由 SQL 线程从 relay log 重放事件，更新数据。
> - 给主库返回响应。
>
> ![MySQL MVCC](/img/0007-主从同步_异步.png "MySQL MVCC示意图")
>
> 概述：主库提交事务会写 binlog，会由一个 dump 线程推送给从库，从库接收之后会有一个 I/O 线程将其写到 relay log 中，慢慢消化，由 SQL 线程来重放更新数据。
>
> 异步复制有数据丢失风险，例如数据还未同步到从库，主库就给客户端响应，然后主库挂了，此时从库晋升为主库的话数据是确实的。
>
> **同步复制**
>
> 主库需要将 binlog 复制到所有从库，等所有从库响应了之后才会给客户端响应，这样的话性能很差，一般不会选择同步复制.
>
> **半同步复制**
>
>  MySQL 5.7 之后搞了一个半同步复制,有个参数可以选择"成功同步几个从库就返回响应."
>
> 如:一共有 3 个从库,我参数配置 1 , 那么只要有一个从库响应说复制成功了,主库就直接返回响应给客户端, 不会等待其他两个从库. 这样的话性能就比较好, 并且数据可靠性也增强了, 只有当那个从库和主库同时挂了, 才会缺失数据. 



## 如何处理 MySQL 的主从同步延迟?

首先需要明确一个点 延迟是必然存在的, 无论怎么优化都无法避免延迟的存在, 只能减少延迟时间.

常见的解决方式有以下几种:

- **二次查询**: 如果从库查不到数据, 则再去主库查一遍, 由 API 封装这个逻辑即可, 算是一个兜底策略, 比较简单. 不不过等于读的压力又转移到主库身上, 如果又不法分子故意查询必定查不到的查询, 这就对主库产生了冲击了.
- **强制将写之后立马读的操作转移到主库上:** 这种属于代码写死了, 比如一些写入之后立马查询的操作, 就绑定在一起, 写死都走主库. 不推荐, 比较死板.
- **关键业务读写都走主库:** 非关键还是读写分离. 比如上面我举例的用户注册这种, 可以读写主库, 这样就不会有登录报该用户不存在的问题, 这种访问量频次应该也不会很多, 所以看业务适当调整此类接口.
- **使用缓存:** 主库写入后同步到缓存中, 这样查询时可以先查询缓存,避免了延迟的问题, 不过又引入了缓存数据一致性的问题. 



# 6. MySQL 知识点 --- 优化问题                

## MySQL 的 EXPLAIN 语句进行查询分析如何使用？

![MySQL MVCC](/img/0006-mysql_explain.png "MySQL MVCC示意图")



## MySQL 中如何进行 SQL 调优？

平时进行 SQL 调优，主要是通过观察慢 SQL，然后利用 explain 分析查询语句的执行计划，识别性能瓶颈，优化查询语句:

- 合理设计索引，利用联合索引进行覆盖索引的优化，避免回表的发生，减少一次查询和随机 I/O 。
- 避免 SELECT*，只查询必要的字段。
- 避免在 SQL 中进行函数计算等操作，使得无法命中索引。
- 避免使用 %LIKE，导致全表扫描。
- 注意联合索引需满足最左匹配原则。
- 不要对无索引字段进行排序操作。
- 连表查询需要注意不同字段集是否一致，否则也会导致全表扫描。

除此之外，还可以 **利用缓存** 来优化，一些变化少或者访问频繁的数据设置到缓存中，减少数据库的压力，提升查询的效率。

还可以 **通过业务** 来优化，例如少展示一些不必要的字段，减少多表查询的情况，将列表查询替换成分页分批查询等待。

> 扩展知识：
>
> **慢 SQL：**
>
> 这是 MySQL 自带的日志记录，默认关闭，通过 set global slow_query_log = 'ON' 即可开启。
>
> 通过 **show variables like '%slow_query_log%**' 即可查询当前慢日志是否开启，以及存储的路径。
>
> 通过 **set global long_query_time = 3** 即可设置慢 SQL 的阈值，3 就是 3秒，当一个 SQL 执行的时间操作 3 秒，就会被记录到慢日志中。
>
> **explain：**
>
> 通过 explain 命令分析查询 SQL



## 从 MySQL 获取数据, 是从磁盘读取的吗?

在 MySQL 中,获取数据 并不总是直接从磁盘读取. MySQL 使用缓存机制, 比如 InnoDB 存储引擎,会将常用的数据和索引缓存在内存中, 以提高读取性能. 当查询数据时, 系统首先会检查缓存(如:缓冲池), 如果数据存于内存中, 则直接从内存中读取; 如果不在, 则会从磁盘读取并加载到缓存中.

> 扩展知识
>
> 详细请看: https://www.mianshiya.com/bank/1791003439968264194/question/1780933295555506177
>
> MySQL 从缓存中读取所指的缓存, 实际上包含两个缓存:
>
> - 查询缓存(**MySQL 8.0 已废弃**)
> - InnoDB 缓冲池(buffer pool)
>
> ![MySQL MVCC](/img/0008-sql执行流程图.png "MySQL MVCC示意图")



## 为什么在 MySQL 中不推荐使用多表 JOIN？

**性能问题：**

- 多表 JOIN 可能导致查询性能下降，尤其是在处理大数据集时，JOIN 操作的计算复杂度会显著增加，需要进行大量的数据扫描和匹配，增加了内存和CPU的消耗，导致响应时间变长。

**可读性和维护性：**

- 多表 JOIN 的查询语句较为复杂，降低了 SQL 的可读性和可维护性。复杂的语句可能会增加错误发生的概率，使得后续的调试和优化更加困难。

> 扩展知识
>
> **多表 JOIN**
>
> - 这里多表往往指的是超过三个表才是多表，正常两个表 join 是没问题的！（但是也需要评估下量级和是否会命中索引）。
> - **在阿里的 Java 规范手册里也又一句话：“超过三个表禁止使用 join”。**
> - **数据库往往是我们系统的弱点**，很多情况下性能瓶颈都在数据库，**因此我们需要尽量避免把压力放在数据库上**。
> - JOIN 使用的时候 **尽量让小表驱动大表**，因为作为驱动表需要进行全表扫描，而被驱动表是通过索引查询的。（好像是左边的表是驱动表）
>
> 案例详情请看：https://www.mianshiya.com/bank/1791003439968264194/question/1780933295568089090



## MySQL 中如何解决深度分页的问题？

> 直接看详情吧：https://www.mianshiya.com/bank/1791003439968264194/question/1780933295572283394

> 扩展知识：
>
> **深度分页**
>
> - 所谓的深度分页是指数据量很大的时候，按照分页访问后面的数据，例如 **limit 999999990，10** ，这会使得数据库扫描前面的 999999990 条数据，才能得到最终的 10 条数据，大批量的扫描数据会增加数据库的负载，影响性能。



## 如何在 MySQL 中监控和优化慢 SQL？

可以利用 MySQL 自带的 slow_query_log 来监控慢 SQL ，它是 MySQL 提供的一个日志功能，用于记录执行时间超过特定阈值的 SQL 语句。

对于慢查询，再使用 EXPLAIN 分析执行计划，查看查询的执行顺序、使用的索引、扫描的行数等，以识别潜在的性能瓶颈。

基于 EXPLAIN 再进行针对性的优化，常见的优化方向有：

- 根据 EXPLAIN 的结果，检查是否有合适的索引。若缺失索引，则添加（特别是在 WHERE、JSON 和 ORDER BY 子句中使用的列上）。
- 将复杂的 JOIN 查询拆分为多个简单的查询，尽量小表驱动大表。
- 避免 SELECT*，仅选择需要的字段。
- 等等（更多可参考扩展中的 SQL 调优）。

> 扩展知识
>
> **慢 SQL 配置文件详情见：**https://www.mianshiya.com/bank/1791003439968264194/question/1780933295572283395



## MySQL 中 ’LIMIT 100000, 10' 和 ‘LIMIT 10’ 的执行速度是否相同？

速度差很多哦，可以见之前的那个深度分页的问题。

原因：

- **LIMIT 100000, 10** 需要先处理（通常是读取并跳过）前100000条记录，然而再获取到需要的 10 条记录，开销成本很大，因为需要扫描 100000 数据才能得到后面的 10 条数据，会导致大量的磁盘 I/O 。
- **LIMIT 10** ,直接从结果集第一个记录开始扫直接返回前 10 条记录。

通常大分页的解决方案如下：

```sql
SELECT * FROM users WHERE id > 100000 ORDER BY id LIMIT 10;
```



## MySQL 数据库的性能优化方法有哪些？

> 总结：
>
> - SQL 层面：优化查询语句、避免全表扫描、使用 EXPLAIN 分析工具
> - 索引 层面：创建索引、避免索引失效、索引维护和重建
> - 库表 层面：范式化设计、数据类型优化、分区设计、主从读写分离、分库分表、选择适合的表引擎
> - 缓存优化：数据库缓存（Redis、Memcached等）、应用层缓存、缓存预热
> - 硬件优化：采用SSD、磁盘阵列
> - 并发控制优化：并发连接数（合理）、锁机制
> - 监控与调优：使用监控工具、调整配置和参数、性能测试和压力测试

对于开发人员，可以从 **SQL** 和 **库表** 设计两部分优化 MySQL 性能问题

**SQL 优化**

根据 SQL 日志，找出需要优化的一些语句。

常见优化方向：

- 避免 SELECT* ，只查询必要的字段
- 避免在 SQL 中进行函数计算等操作，使得无法命中索引
- 避免使用 %LIKE，导致全表扫描
- 注意联合索引需要满足最左匹配原则
- 不要对无索引字段进行排序操作
- 连表查询需要注意不同字段的字符集是否一致，否则也会导致全表扫描

**库表设计优化**

- 合理的表结构：比如选择合适的数据类型，例如能用 int 的不要用 bigint，还有 varchar 和 char 的选择等等
- 合理冗余字段：在适当的情况下进行反规范化设计，冗余部分数据，减少关联查询
- 索引优化：根据查询频率和条件，创建合适的索引，删除不必要的索引，因为索引的维护也是需要成本的，建议使用 EXPLAIN 分析查看执行计划，确认是否用上索引，是否用对索引。
- 分库分表：对于超大规模的数据库系统，可以采用分库分表策略，将数据拆分到多个数据库或表中，提高读写性能和扩展性。

> 扩展知识
>
> **MySQL 相关配置优化**
>
> 可以根据具体的使用场景调整以下参数：
>
> - innodb_buffer_pool_size：增大 InnoDB 的缓存池大小，一般设置为物理内存的 70~80%
> - query_cache_size：适当调整查询缓存大小
> - max_connerctions：增大最大连接数
> - table_open_cache：增加打开表的缓存大小
> - thread_cache_size：调整线程缓存大小以减少线程创建的开销
>
> **硬件层面的优化**
>
> - 升级硬件：增加服务器的内存、CPU 和 存储速度（现在云厂商都支持不停服直接升级）
> - 使用 SSD：相比传统的 HDD，SSD 读取和写入速度更快
>
> **数据库维护**
>
> - 定期备份：保存数据安全，防止数据丢失
> - 定期清理：删除不必要的数据和日志，释放存储空间（例如历年的数据可以剥离存档，释放当前表的大小）
> - 重建索引：定期重建索引，保持索引的高效性
> - 分析表和优化表：定期运行 ANALYZE TABLE 和 OPTIMIZE TABLE ，保持表的统计信息更新和碎片整理，使得优化器更加精准。
>
> **缓存**
>
> 数据库始终是有瓶颈的，如果不论怎么优化性能上不去，此时可以考虑用缓存。在部分场景可以使用本地缓存和 Redis 分布式缓存减轻 MySQL 查询压力。
>
> 但是要关注缓存和数据库的一致性问题。



## MySQL 的查询优化器如何选择执行计划（简）？

MySQL 优化器会从多个优化执行中，选择一个成本最低计划执行。

其中成本包括：IO成本、CPU成本

IO成本：将一页的数据从磁盘加载到内存的成本

CPU成本：数据读取到内存后，需要比较和排序等操作，此时需要占用CPU资源。



------



# 7. MySQL 知识点 --- 基础知识

## 数据库的三大范式是什么？

1. 第一范式（1NF）:

- 确保每个列的值都是原子值，表中的每个字段只能包含单一的数据项，**不允许重复的列和多值字段。**

2. 第二范式（2NF）:

- 在满足第一范式的基础上，确保表中的每个非主键字段完全依赖于主键，而不是部分依赖。即，**非主键字段必须依赖于整个主键**。

3. 第三范式（3NF）：

- 在满足第二范式的基础上，确保非主键字段之间不依赖，即消除传递依赖。**所有非主键字段只能依赖于主键，不应相互依赖**。

> 扩展知识
>
> **三大范式进一步理解**
>
> 数据库的三大范式是数据库设计中常用的规范，它们目的是减少数据冗余，提高数据的完整性和一致性，使得表的设计更清晰。
>
> - **第一范式（1NF）：规范化**
>
> **目的**：确保数据表的每一列都是单一值，消除重复的列，从而保证数据的原子性。
>
> 例如地址作为一个字段，实际上可以拆分成省、市、区等，所以这就不符合第一范式。
>
> - **第二范式（2NF）：消除部分依赖**
>
> **目的**：消除非主键字段对主键部分依赖，从而避免数据冗余和更新异常。
>
> 例如【员工ID、员工姓名、部门ID、部门名】为一张表，员工 ID 为主键。此时，员工姓名依赖员工 ID，部门名依赖部门 ID ，这就违反了第二范式。
>
> 符合范式的设计是：员工表【员工 ID 、员工姓名、部门 ID】，部门表【部门 ID、部门名】
>
> - **第三范式（3NF）:消除传递依赖**
>
> **目的：**消除非主键字段对主键的传递依赖，从而进一步减少数据冗余和更新异常。
>
> 例如【员工ID、员工姓名、部门ID、部门名、经理ID】为一张表，员工 ID 为主键。此时，经理 ID 依赖部门 ID ，部门 ID 依赖员工 ID，这说明有依赖传递，违反了第三范式。
>
> 例如【员工ID、员工姓名、部门ID、部门名、经理ID】为一张表，员工 ID 为主键。此时，经理 ID 依赖部门 ID，部门 ID 依赖员工 ID，这说明有依赖传递，违反了第三范式。
>
> 符合范围的设计是：员工表【员工ID、员工姓名、部门ID】,部门表【部门ID、部门名、经理ID】。
>
> **小结**
>
> - 第一范式（1NF）：确保每一列都是原子值，即是不可分割的基础数据项。
> - 第二范式（2NF）：在满足 1NF 的基础上，消除非主键字段对主键部分的依赖。
> - 第三范式（3NF）：在满足 2NF 的基础上，消除非主键字段对主键的传递依赖。
>
> **不过现在的业务上的表设计基本上都是反范式的**。当然不是说完全不遵守范式，而是适当的进行调整。
>
> 比如业务上经常需要冗余字段，减少联表查询，提升性能，特别是业务量比较大的公司，这种冗余是很有必要的！



## MySQL 中的函数你用过哪些？

- 字符串函数，用于处理文本数据：
  - concat：连接字符串
  - substring：提取子字符串
  - length：返回字符串的长度
  - replace：替换字符串中的子字符串
  - upper 和 lower：将字符串转换为大写或小写
  - trim：取出字符串两端的空格
  - left 和 right：返回字符串左边或右边的字符
- 数学函数，用于处理数字运算：
  - abs：返回绝对值
  - ceil 和 floor：返回大于或等于/小于或等于指定数的最小整数/最大整数。
  - mod：返回除法的余数
  - power：返回一个数的指定次幂
- 日期函数，用于处理日期和时间：
  - now：返回当前日期和时间
  - date_add 和 date_sub：日期加上或减去一个时间间隔
  - datediff：计算两个日期之间的差异
  - year，month，day：提取日期的年份、月份、日期
  - str_to_date：将字符串转换为日期
- 聚合函数，用于汇总数据：
  - count：计算行数
  - sum：计算总数
  - avg：计算平均值
  - max 和 min：返回最大值和最小值
- 条件函数，用于实现条件逻辑：
  - if：条件判断函数
  - ifnull：返回第一个非 null 值
  - case：条件选择器



## 什么是数据库的视图？

数据库的视图是一个 **虚拟表** ，它并不存储实际的数据，而是通过查询其他表数据来生成的。视图可以简化复杂的查询、增加数据安全性（限制访问某些数据）以及提供数据的不同表示方式。

它可以包含一个或多个表的数据，并且可以对这些数据进行筛选、计算、排序等操作。

视图不会占用物理空间，只是一种逻辑上的表示。

> 扩展知识
>
> **视图的作用**
>
> - **简化复杂查询：**视图可以将复杂的查询封装成一个简单的视图，使得用户在查询数据时更加方便。
> - **安全性：**通过视图可以限制用户访问特定的表和列，保护敏感数据。例如，只允许用户查看某些列而不是整个表。
> - **数据抽象：**视图提供了一种数据抽象层，用户可以通过视图获取需要的数据，而不必关心底层表的结构和关系。
> - **可重用性：**定义一次视图，可以在多个查询中重复使用，减少代码冗余。
>
> 创建视图：
>
> ```sql
> CREATE VIEW employee_salaries AS
> SELECT 
>     e.employee_id,
>     CONCAT(e.first_name, ' ', e.last_name) AS full_name,
>     d.department_name,
>     e.salary
> FROM 
>     employees e
> JOIN 
>     departments d ON e.department_id = d.department_id;
> ```
>
> 使用视图：
>
> ```sql
> SELECT * FROM employee_salaries;
> ```



------



# 8. MySQL 知识点 --- 日志

## 什么是 Write-Ahead Logging（WAL）技术？它的优点是什么？MySQL 中是否用道理 WAL？

> 简介流程：
>
> 查询数据到内存 ---> 记录 undo log ---> 记录 redo log（预提交）---> 更新内存 ---> 记录 binlog ---> 记录 redolog（已提交）

WAL（Write-Ahead Logging）技术是一种数据库事务日志管理技术，它确保在修改真正的数据之前，先将修改记录写入日志。这使得即使系统崩溃，通过日志也能恢复数据，保证了数据的持久性和一致性。

WAL 它的核心思想就是 **先写日志，再写数据** 大致执行流程如下：

- 当一个事务开始时，所有对数据库的修改都会记录到一个日志文件中，而不是直接应用到数据库文件中，这些日志记录了数据的变更信息，可以用于恢复数据。
- 当日志记录被安全地写入磁盘后，才会将这些修改应用到数据库文件中。

> WAL 优点
>
> - 保证数据一致性：再数据提交之前，变更首先记录到日志中，在系统崩溃后，数据库可以通过重做日志中的操作来恢复到崩溃前的状态，确保数据一致性和持久性。
> - 性能提升：把数据的随机写转化为日志的顺序写，提高了整体性能。
>
> **Redo Log**
>
> 在 MySQL InnoDB 存储引擎中，重做日志（Redo Log）就是 WAL 的实现，用于保证事务的持久性和崩溃恢复能力。
>
> InnoDB 重做日志的工作机制如下：
>
> - 当一个事务开始时，所有对数据库的修改首先记录到重做日志缓冲区中。
> - 重做日志缓冲区的数据会周期性地刷新到磁盘上的重做日志文件（ib_logfile0 和 ib_logfile1）。
> - 当事务提交时，InnoDB 确保重做日志已写入磁盘，然后将数据页的修改写入数据文件。
> - 如果系统崩溃，InnoDB 会在启动时通过重做日志重新应用所有未完成的事务，以恢复数据库到一致状态。





------



# 9. MySQL 知识点 --- 其它 

## MySQL 中 DELETE、DROP 和 TRUNCATE 的区别是什么？

- **Delete** 用于删除行数据，但保留表结构和相关的对象。
- **Drop** 用于完全删除数据库表，包括数据结构。
- **Truncate** 只删除数据，不会删除表结构和索引等其他结构。

从性能来看，**DROP > Truncate > Delete**

> 扩展知识
>
> **Delete**
>
> 本质上这个删除其实就是给数据行打个标记，并不实时删除，因此 delete 后，空间的大小不会变化。
>
> 而且 delete 操作会生成 binlog、redlog 和 undolog，所以如果删除全表使用 delete 的话，性能会比较差！但是它可以回滚。
>
> **Drop**
>
> 在 InnoDB 中，每张表数据的内容和索引都存储在一个 .ibd 后缀的文件中，drop 就是直接把这个文件给删除了！还有一个 .fm 后缀的文件也会被删除，这个文件包含的元数据和结构定义。
>
> 文件都删除了，所以这个操作无法回滚，表空间会被回收，但是如果表存在系统共享表空间，则不会回收空间。
>
> 默认创建的表会有独立表空间，把 **innodb_fil_per_table** 的值改为 OFF 后，就会被放到共享表空间中，即统一的 ibdata1 文件中。
>
> **Truncate**
>
> Truncate 会对整张表的数据进行删除，且不会记录回滚等日志，所以它无法被回滚。
>
> 并且主键字段是自增的，使用 Truncate 删除后自增重新从 1 开始。



## MySQL 中 INNER JOIN、LEFT JOIN 和 RIGHT JOIN 的区别是什么？

- **INNER JOIN**   内连接
  - 只返回两个表中匹配的行。如果没有匹配，则该行不会出现在结果集中。
  - 适用于只关心交集数据的场景。
- **LEFT JOIN (或 LEFT OUTER JOIN) ** 左连接
  - 返回左表中的所有行，即使右表中没有匹配的行。如果右表没有匹配，则结果中的右侧列会显示为 NULL。
  - 适用于需要保留左表所有数据的场景。
- **RIGHT JOIN (或 RIGHT OUTER JOIN)**  右连接
  - 返回右表中的所有行，即使左表中没有匹配的行。如果左表没有匹配，则结果中的左侧列会显示为 NULL。
  - 适用于需要保留右表所有数据的场景。



## MySQL 中 DATETIME 和 TIMESTAMP 类型的区别是什么？

- 存储方式：
  - DATETIME：以字符串形式存储，范围为 **`1000-01-01 00:00:00`** 到 **`9999-12-31 23:59:59`** ，占用 8 字符。
  - TIMESTAMP：以 Unix 时间戳形式存储，范围为 **`1970-01-01 00:00:00`** UTC 到 **`2038-01-19 03:14:07`** UTC ,占用 4 字节。
- 时区处理：
  - DATETIME：不受时区影响，存储的时间是具体的日期和时间，不能进行自动转换。
  - TIMESTAMP：受时区影响，存储时会转换为 UTC，取出时会根据连接的时区进行转换，适合处理跨时区的数据。

> 扩展知识：
>
> **默认值和自动更新**
>
> - DATETIME：在 MySQL 5.6 及更早版本中，DATETIME 列不能有自动更新的默认值。在 MySQL 5.6 及以后版本中，可以使用 DEFAULT 和 ON UPDATE 子句来指定自动初始化和更新行为，但不像 TIMESTAMP 那么直观。
> - TIMESTAMP：在 MySQL 5.6 及更高版本中，TIMESTAMP 列可以有默认的当前时间戳 CURRENT_TIMESTAMP，并且可以使用 ON UPDATE CURRENT_TIMESTAMP 使其在行更新时自动更新为当前时间戳。这使得 TIMESTAMP 非常适合记录行的创建和修改时间。



## MySQL 中 TEXT 类型最大可以存储多长的文本？

- tinytext：最大长度255字节**（2^8 - 1）**
- text：最大长度为 63535 字节，约 64 KB **（2^16 -1）**
- mediumtext：最大长度为 16777215 字节，约 16 MB  **(2^24 -1)**
- longtext：最大长度为 4294967295 字节，约 4 GB  **(2^32 - 1)**



## MySQL 中 AUTO_INCREMENT 列达到最大值会发生什么？

在 MySQL 中，**如果表定义的自增 ID 到达上限后，再申请下一个 ID，得到的值不变！**因此会导致报重复值的错误。

> 扩展知识
>
> **AUTO_INCREMENT 列不同数据类型的最大值：**
>
> - tinyint（8位）：最大值是 127 （有符号）或 255（无符号）
> - smallint（16位）：最大值 32767（有符号） 或 65535 （无符号）
> - mediumint（24位）：最大值 8388607 （有符号）或 16777215 （无符号）
> - int（32位）：最大值 2147483647（有符号）或 4294967295（无符号）
> - bigint（64位）：最大值 9223372036854775807（有符号）或 18446744073709551615（无符号）
>
> **如果 InnoDB 表没有配置主键，有最大值上限吗？**
>
> - 如果不配置主键，InnoDB 会默认创建一个不可见的长度为 6 个字节的 row_id
> - InnoDB 在全局维护了一个 **`dict_sys，row_id`** 值，所有需要用到 row_id 的表，每次插入一行数据，都会获取这个值，然后将其 +1
> - **了解**：这个值的范围是 0~2^48 -1 ，如果这个值达到上限后，又会从 0 开始，然后继续循环，**如果插入的新数据 row_id 在表中已存在，那么老数据会被这个新数据覆盖**，不会产生任何报错。



## 在 MySQL 中存储金额数据，应该使用什么数据类型？

在数据库中业界常用两种类型来存储金额：bigint 和  decimal

- bigint 代码中用 Long
  - 范围：可以存储的整数范围为 -2^63 到 2^63 -1 （在 MySQL 中为 64 位有符号整数）
  - 存储空间：占用 8 字节（64位）
  - 精度：精确存储整数，蛋不支持小数部分，存储的金额单位是分
- decimal 代码中使用 BigDecimal
  - 范围：可以存储的数字范围和小数位数由定义的精度和标度决定
  - 存储空间：存储空间取决于定义的精度和标度，存储较大数值会占用更多空间
  - 精度：支持高精度的小数运算，精确存储定点数，一般用 decimal(18,6)，18 是总位数，6 是小数



## 为什么不推荐在 MySQL 中直接存储图片、音频、视频等大容量内容？

MySQL 是关系型数据库，**它设计的初衷是高效处理结构化和关系型数据**，所以存储大容量的内容本身就不是它的职责所在，因此这方面的能力也不变。

应该将大容量文件存储在文件系统或云服务提供的对象存储服务中，尽在数据库中存储文件的路径或 URL 即可。

> 扩展知识
>
> - 数据库本身性能问题：
>   - 数据库性能：存储检索大容量的二进制数据（如图片、音频、视频）会显著增加数据库的 I/O 操作和处理时间，从而影响性能。
>   - 查询性能：大容量的二进制数据会增加数据库表的大小，导致查询性能下降。尤其是在设计表连接或复杂查询时，性能影响更大。
> - 可扩展性问题：
>   - 文件系统可以通过分布式文件系统（如 Hadoop HDFS 、Amazon S3）更容易地进行扩展，而数据库扩展则复杂得多。
> - 权限控制问题：
>   - 像文件系统和对象存储系统（如 Amazon S3、OSS 等）通常提供更灵活的访问控制和权限控制，适合处理大容量数据的存储和访问；例如访问的时效性等等，而 MySQL 不提供这些功能，需要我们应用程序额外开发。



## 如何实现数据的不停服迁移？

数据迁移，**不停机**迁移

**实际分细节：**

- 首先关注量级，**如果是几十万的数据其实直接用代码迁移，简单核对下就结束了**，如果数据量大那么菜需要好好设计方案。
- 不停服数据迁移需要考虑数据的插入和修改，保证数据的一致性。
- 迁移还需要注意回滚，因为一旦发生问题需要及时切换回老库，防止对业务产生影响。

**双写方案**

大部分数据库迁移都会采用双写方案，例如自建的数据库要迁移到云上的数据库这个场景，双写就是同时写入自建的数据库和云上的数据库。

**迁移流程：**

1. 将云上数据库（新库）作为自建数据库（旧库）的从库，进行数据同步（或者可以利用云上的能力，比如阿里云的DTS）。
2. 改造业务代码，数据写入修改不仅要写入旧库，同时也要写入新库，这就是所谓的双写，注意这个 **双写需要加开关**，即通过修改配置实时打开双写和关闭双写。
3. **在业务低峰期**，确保数据同步完全一致的时候（即主从不延迟，这个都是有对应的监控的），关闭同步，同时打开双写开关，此时业务代码读取的还是旧数据库。
4. 进行数据核对，数据量很大的场景只能 **抽样调查** （可以利用定时任务写代码进行抽样核对，一旦不一致就告警和记录）。
5. 如果确认数据一致，此时可以进行 **灰度切流**，比如 1% 的用户切到读新的数据库（比如今天访问前 1% 的用户或者根据用户 ID 或其他业务字段），如果发现没问题，则可以逐步增加开放的比例，比如 5% -> 20% -> 50% -> 100%
6. 继续保留双写，跑个几天（或者更久），确保新库缺失没问题了，此时关闭双写，只写新库，这时候迁移就完成了.

**Flink CDC 方案**

除了主从同步,代码双写的方案,也可以采用第三方工具,例如 flink-cdc 等工具来进行数据的同步,它的优点方便,且支持异构(比如 mysql 同步到 pg  | es 等等)的数据源

![](/img/0016-flink-cdc.png)

像 flink-cdc 支持先同步全量历史数据，再无缝切到同步增量数据。上图中蓝色小块就是新增的插入数据，会追加实时一致性快照中；上图中黄色小块是更新的数据，则会在已有历史数据里面做更新。



## MySQL 中 InnoDB 存储引擎与 MyISAM 存储引擎的区别是什么（简）？

|              | InnoDB                               | MyISAM                             |
| ------------ | ------------------------------------ | ---------------------------------- |
| 数据结构     | 索引组织表（数据即索引，索引即数据） | 堆表（索引和数据是两个不同的文件） |
| 数据存放     | 叶子节点上存储的是索引和数据行       | 叶子节点上存储的是索引和数据地址   |
| 事务         | 支持                                 | 不支持                             |
| 行级锁       | 支持                                 | 不支持                             |
| 数据崩溃恢复 | 支持（有 redolog日志）               | 不支持                             |

适用场景：

- InnoDB 更适合需要高并发、事务处理和数据完整性保证的场景（电商平台、金融系统和社交网络等）
- MyISAM 更适合读操作远多于写操作且堆数据完整性要求不高的场景（内容管理系统、博客平台和报表系统等）



## MySQL 事务的二阶段提交是什么？

MySQL 事务的二阶段提交是指在 MySQL 中，为了确保 redo log（重做日志）和 binlog（二进制日志）之间的一致性，使用的一种机制，MySQL 通过二阶段提交来保证在 crash recovery（崩溃恢复）时，不会出现 **数据丢失** 或 **数据不一致** 的情况。

**二阶段提交的两个阶段：**

- **准备阶段（Prepare Phase）**在事务提交时，MySQL 的 InnoDB 引擎会先写入 redo log，并将其状态标记为 prepare，表示事务已经准备提交但还未真正完成。此时的 redo log 是预提交状态，还未标记未完成提交。
- **提交阶段（Commit Phase）**当 redo log 的状态变为 prepare 后，MySQL Server 会写入 binlog（记录用户的 DML 操作）。binlog 写入成功后，MySQL 会通知 InnoDB，将 redo log状态改为 commit，完成整个事务的提交过程。



## MySQL 三层 B+ 树能存多少数据？

在 MySQL 的 InnoDB 存储引擎中，B+ 树默认数据页大小为 16KB

**参数：**

- 每个节点页大小为 16KB（即 16384 字节）
- 假设每个数据记录的主键和数据大小为 1KB（一般会比这个小，但这里取整方便计算）
- 每个内部节点（非叶子节点）存储的是指向叶子节点的指针和索引键

**三层 B+ 树的存储计算：**

- 叶子节点：第三层为叶子节点，每个叶子节点页可存储 16 条数据记录（16KB / 1KB）
- 第二层（中间层）：假设每个指针 6字节 和索引键（一般为 bigint）的大小为 8字节，那么每个中间件节点页可以指向 1170 个叶子节点（16KB * 1024 / (6 + 8) 字节）
- 第一层（根节点）：根节点可以指向 1170 个中间节点

由此，三层 B+ 树大致能存储的数据总量为：1170 * 1170 / 16 = 21902400，一棵三层的 B+ 树在 MySQL 中可以存储大约 2000 万条记录。

这里要注意这个只是估算值，估计数量会因实际的数据大小、页大小等因素略有不同。

> 扩展知识
>
> **InnoDB 中页的大小**
>
> 在 InnoDB 中，B+ 树的每个节点通常对应一个 页（Page），默认页大小为 16KB. 页的大小可以通过调整参数 **`innodb_page_size`** 来修改（通常为 4KB、8KB 或 16 KB）
>
> 注意：在 MySQL 中，B+ 树叶子节点存储的是数据行的完整信息，包括主键和其他信息。
>
> 每条记录（即每个数据行） 在 B+ 树的叶子节点中按照主键顺序存储，这使得 InnoDB 的 B+ 树既支持高效的单记录查询，也支持范围查询。

------

> 友情提示：该部分所有知识点来源于鱼皮的 **面试鸭**  + 个人
>
> 侵权联删，谢谢，嘿嘿~
>
> **面试鸭**网址：https://www.mianshiya.com/